{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## SimCLR v1 Architechture"
      ],
      "metadata": {
        "id": "VwCu6ihAuubW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inputs"
      ],
      "metadata": {
        "id": "9bN06ek1FFLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torchlars"
      ],
      "metadata": {
        "id": "qQTTPig8Gsd9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch torchvision"
      ],
      "metadata": {
        "id": "Y4VvCzWrIp-9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torchlars --no-cache-dir"
      ],
      "metadata": {
        "id": "BWEBQ5hYIw9C"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/kakaobrain/torchlars.git\n",
        "# %cd torchlars\n",
        "# !python setup.py install"
      ],
      "metadata": {
        "id": "OBA83iW2JKXG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from torchlars import LARS"
      ],
      "metadata": {
        "id": "aJ4aEdq8Jdxx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pytorch-lightning\n",
        "# !pip install lightning-flash"
      ],
      "metadata": {
        "id": "O98KsSUbiTaA"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning==1.8.3.post0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnYY2im6kBp9",
        "outputId": "6aa52189-eb1d-4c40-a17b-0b436ed4b5bf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch-lightning==1.8.3.post0 in /usr/local/lib/python3.9/dist-packages (1.8.3.post0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.8.3.post0) (1.22.4)\n",
            "Requirement already satisfied: lightning-utilities==0.3.* in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.8.3.post0) (0.3.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.8.3.post0) (0.10.3)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.8.3.post0) (6.0)\n",
            "Requirement already satisfied: tensorboardX>=2.2 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.8.3.post0) (2.6)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.8.3.post0) (2023.3.0)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.8.3.post0) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.8.3.post0) (4.5.0)\n",
            "Requirement already satisfied: torch>=1.9.* in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.8.3.post0) (1.13.1+cu116)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.8.3.post0) (23.0)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.9/dist-packages (from lightning-utilities==0.3.*->pytorch-lightning==1.8.3.post0) (0.5.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.9/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning==1.8.3.post0) (3.8.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning==1.8.3.post0) (2.27.1)\n",
            "Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.9/dist-packages (from tensorboardX>=2.2->pytorch-lightning==1.8.3.post0) (3.19.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.3.post0) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.3.post0) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.3.post0) (1.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.3.post0) (2.0.12)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.3.post0) (22.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.3.post0) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.3.post0) (6.0.4)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.9/dist-packages (from fire->lightning-utilities==0.3.*->pytorch-lightning==1.8.3.post0) (2.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from fire->lightning-utilities==0.3.*->pytorch-lightning==1.8.3.post0) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning==1.8.3.post0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning==1.8.3.post0) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning==1.8.3.post0) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "# from torchlars import LARS  # Optimizer\n",
        "from flash.core.optimizers import LARS\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "hEi10J3QFEwn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Settings"
      ],
      "metadata": {
        "id": "w5PCrod8FH_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "RANDOM_SEED = 1\n",
        "LEARNING_RATE = 0.0001\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "# Architecture\n",
        "NUM_FEATURES = 28*28\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "# LOSS\n",
        "TEMPERATURE = 0.5\n",
        "\n",
        "# Other\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"using : {DEVICE}\")\n",
        "\n",
        "GRAYSCALE = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlIY_YltFKUC",
        "outputId": "09978e07-f6a4-4a97-a1a3-83bd4658e010"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using : cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResNet50 - Base Encoder"
      ],
      "metadata": {
        "id": "U8237ciDu0fx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "iZJtVMLjuDB8"
      },
      "outputs": [],
      "source": [
        "from re import X\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        print(out.shape)\n",
        "        out = self.bn1(out)\n",
        "        print(out.shape)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        print(out.shape)\n",
        "\n",
        "        out = self.bn2(out)\n",
        "        print(out.shape)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        print(out.shape)\n",
        "        out = self.bn3(out)\n",
        "        print(out.shape)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes, grayscale):\n",
        "        self.inplanes = 64\n",
        "        if grayscale:\n",
        "            in_dim = 1\n",
        "        else:\n",
        "            in_dim = 3\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_dim, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, (2. / n)**.5)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        print(x.shape)\n",
        "        x = self.bn1(x)\n",
        "        print(x.shape)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        print(x.shape)\n",
        "        x = self.layer2(x)\n",
        "        print(x.shape)\n",
        "        x = self.layer3(x)\n",
        "        print(x.shape)\n",
        "        x = self.layer4(x)\n",
        "        print(x.shape)\n",
        "        # because MNIST is already 1x1 here:\n",
        "        # disable avg pooling\n",
        "        #x = self.avgpool(x)\n",
        "        \n",
        "        x = x.view(x.size(0), -1)\n",
        "        print(x.shape)\n",
        "        logits = self.fc(x)\n",
        "        probas = F.softmax(logits, dim=1)\n",
        "        return logits, probas\n",
        "\n",
        "\n",
        "\n",
        "def resnet50(num_classes):\n",
        "    \"\"\"Constructs a ResNet-50 model.\"\"\"\n",
        "    model = ResNet(block=Bottleneck, \n",
        "                   layers=[3, 4, 6, 3],\n",
        "                   num_classes=NUM_CLASSES,\n",
        "                   grayscale=GRAYSCALE)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##########################\n",
        "### MODEL\n",
        "##########################\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes, grayscale):\n",
        "        self.inplanes = 64\n",
        "        if grayscale:\n",
        "            in_dim = 1\n",
        "        else:\n",
        "            in_dim = 3\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_dim, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, (2. / n)**.5)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        # because MNIST is already 1x1 here:\n",
        "        # disable avg pooling\n",
        "        #x = self.avgpool(x)\n",
        "        \n",
        "        x = x.view(x.size(0), -1)\n",
        "        logits = self.fc(x)\n",
        "        probas = F.softmax(logits, dim=1)\n",
        "        return logits, probas\n",
        "\n",
        "\n",
        "\n",
        "def resnet50(num_classes):\n",
        "    \"\"\"Constructs a ResNet-50 model.\"\"\"\n",
        "    model = ResNet(block=Bottleneck, \n",
        "                   layers=[3, 4, 6, 3],\n",
        "                   num_classes=NUM_CLASSES,\n",
        "                   grayscale=GRAYSCALE)\n",
        "    return model"
      ],
      "metadata": {
        "id": "AC3qyXLEEHVe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLP - Projection Head"
      ],
      "metadata": {
        "id": "wo8Jswf1u39h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP - One Hidden Layer\n",
        "hidden_layer = nn.Linear(2048, 512)\n",
        "output_layer = nn.Linear(512, 128)\n",
        "mlp = nn.Sequential(hidden_layer, \n",
        "                    nn.ReLU(), \n",
        "                    output_layer)\n"
      ],
      "metadata": {
        "id": "Dx_u_jYbvICz"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SimCLR v1 - Model"
      ],
      "metadata": {
        "id": "jrJotVYPAj5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SimCLRv1 Model\n",
        "class SimCLR(nn.Module):\n",
        "    def __init__(self, base_encoder, projection_dim=128):\n",
        "        super(SimCLR, self).__init__()\n",
        "\n",
        "        self.encoder = base_encoder\n",
        "        self.projection_head = nn.Sequential(\n",
        "            # self.encoder.fc.in_features Ã  la place de 2048\n",
        "            nn.Linear(2048, 2048),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(2048, projection_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = h.view(h.size(0), -1)\n",
        "        z = self.projection_head(h)\n",
        "        return h, z\n"
      ],
      "metadata": {
        "id": "y_Oz7iQt_7YI"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = resnet50(10)\n",
        "y = y(torch.randn(1, 1, 28, 28))\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KYLustun7fzi",
        "outputId": "c8767f6a-bf3b-4d8c-eaf8-26435a97a2a5"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 64, 14, 14])\n",
            "torch.Size([1, 64, 14, 14])\n",
            "torch.Size([1, 64, 7, 7])\n",
            "torch.Size([1, 64, 7, 7])\n",
            "torch.Size([1, 64, 7, 7])\n",
            "torch.Size([1, 64, 7, 7])\n",
            "torch.Size([1, 256, 7, 7])\n",
            "torch.Size([1, 256, 7, 7])\n",
            "torch.Size([1, 64, 7, 7])\n",
            "torch.Size([1, 64, 7, 7])\n",
            "torch.Size([1, 64, 7, 7])\n",
            "torch.Size([1, 64, 7, 7])\n",
            "torch.Size([1, 256, 7, 7])\n",
            "torch.Size([1, 256, 7, 7])\n",
            "torch.Size([1, 64, 7, 7])\n",
            "torch.Size([1, 64, 7, 7])\n",
            "torch.Size([1, 64, 7, 7])\n",
            "torch.Size([1, 64, 7, 7])\n",
            "torch.Size([1, 256, 7, 7])\n",
            "torch.Size([1, 256, 7, 7])\n",
            "torch.Size([1, 256, 7, 7])\n",
            "torch.Size([1, 128, 7, 7])\n",
            "torch.Size([1, 128, 7, 7])\n",
            "torch.Size([1, 128, 4, 4])\n",
            "torch.Size([1, 128, 4, 4])\n",
            "torch.Size([1, 512, 4, 4])\n",
            "torch.Size([1, 512, 4, 4])\n",
            "torch.Size([1, 128, 4, 4])\n",
            "torch.Size([1, 128, 4, 4])\n",
            "torch.Size([1, 128, 4, 4])\n",
            "torch.Size([1, 128, 4, 4])\n",
            "torch.Size([1, 512, 4, 4])\n",
            "torch.Size([1, 512, 4, 4])\n",
            "torch.Size([1, 128, 4, 4])\n",
            "torch.Size([1, 128, 4, 4])\n",
            "torch.Size([1, 128, 4, 4])\n",
            "torch.Size([1, 128, 4, 4])\n",
            "torch.Size([1, 512, 4, 4])\n",
            "torch.Size([1, 512, 4, 4])\n",
            "torch.Size([1, 128, 4, 4])\n",
            "torch.Size([1, 128, 4, 4])\n",
            "torch.Size([1, 128, 4, 4])\n",
            "torch.Size([1, 128, 4, 4])\n",
            "torch.Size([1, 512, 4, 4])\n",
            "torch.Size([1, 512, 4, 4])\n",
            "torch.Size([1, 512, 4, 4])\n",
            "torch.Size([1, 256, 4, 4])\n",
            "torch.Size([1, 256, 4, 4])\n",
            "torch.Size([1, 256, 2, 2])\n",
            "torch.Size([1, 256, 2, 2])\n",
            "torch.Size([1, 1024, 2, 2])\n",
            "torch.Size([1, 1024, 2, 2])\n",
            "torch.Size([1, 256, 2, 2])\n",
            "torch.Size([1, 256, 2, 2])\n",
            "torch.Size([1, 256, 2, 2])\n",
            "torch.Size([1, 256, 2, 2])\n",
            "torch.Size([1, 1024, 2, 2])\n",
            "torch.Size([1, 1024, 2, 2])\n",
            "torch.Size([1, 256, 2, 2])\n",
            "torch.Size([1, 256, 2, 2])\n",
            "torch.Size([1, 256, 2, 2])\n",
            "torch.Size([1, 256, 2, 2])\n",
            "torch.Size([1, 1024, 2, 2])\n",
            "torch.Size([1, 1024, 2, 2])\n",
            "torch.Size([1, 256, 2, 2])\n",
            "torch.Size([1, 256, 2, 2])\n",
            "torch.Size([1, 256, 2, 2])\n",
            "torch.Size([1, 256, 2, 2])\n",
            "torch.Size([1, 1024, 2, 2])\n",
            "torch.Size([1, 1024, 2, 2])\n",
            "torch.Size([1, 256, 2, 2])\n",
            "torch.Size([1, 256, 2, 2])\n",
            "torch.Size([1, 256, 2, 2])\n",
            "torch.Size([1, 256, 2, 2])\n",
            "torch.Size([1, 1024, 2, 2])\n",
            "torch.Size([1, 1024, 2, 2])\n",
            "torch.Size([1, 256, 2, 2])\n",
            "torch.Size([1, 256, 2, 2])\n",
            "torch.Size([1, 256, 2, 2])\n",
            "torch.Size([1, 256, 2, 2])\n",
            "torch.Size([1, 1024, 2, 2])\n",
            "torch.Size([1, 1024, 2, 2])\n",
            "torch.Size([1, 1024, 2, 2])\n",
            "torch.Size([1, 512, 2, 2])\n",
            "torch.Size([1, 512, 2, 2])\n",
            "torch.Size([1, 512, 1, 1])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-127-877d24ae33ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-120-091c3dc8299c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# because MNIST is already 1x1 here:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-120-091c3dc8299c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \"\"\"\n\u001b[0;32m--> 171\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2446\u001b[0m         )\n\u001b[1;32m   2447\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2448\u001b[0;31m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2450\u001b[0m     return torch.batch_norm(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_verify_batch_size\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m   2414\u001b[0m         \u001b[0msize_prods\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2415\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_prods\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2416\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected more than 1 value per channel when training, got input size {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 512, 1, 1])"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "resNet50 = resnet50(NUM_CLASSES)\n",
        "resNet50 = list(resNet50.children())[:-1]\n",
        "resNet50 = nn.Sequential(*resNet50)\n",
        "\n",
        "SimCLRv1_model = SimCLR(resNet50)\n",
        "\n",
        "SimCLRv1_model.to(DEVICE)\n"
      ],
      "metadata": {
        "id": "ZpgDsjU6B-Ti",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47c3a701-c4f4-4fcd-fc06-0ec2a830e206"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimCLR(\n",
              "  (encoder): Sequential(\n",
              "    (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (4): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (5): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (6): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (4): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (5): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (7): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (8): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
              "  )\n",
              "  (projection_head): Sequential(\n",
              "    (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Linear(in_features=2048, out_features=128, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torchvision.models\n",
        "\n",
        "# modelResnet50 = resnet50(10)\n",
        "# modelResnet50.fc.in_features"
      ],
      "metadata": {
        "id": "dvsGC5G8wKcT"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LARS - Optimizer"
      ],
      "metadata": {
        "id": "5ouLWEMFvlC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pytorch-lightning\n",
        "# !pip install lightning-flash"
      ],
      "metadata": {
        "id": "6XCzU7q-M61R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flash.core.optimizers import LARS\n",
        "\n",
        "optimizer = LARS(\n",
        "    SimCLRv1_model.parameters(),\n",
        "    lr=0.1,\n",
        "    weight_decay=1e-6,\n",
        "    momentum=0.9\n",
        ")"
      ],
      "metadata": {
        "id": "el6oQ6x7vn27"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NT-Xent - Constrative Loss\n",
        "*normalized temperature-scaled cross entropy loss*"
      ],
      "metadata": {
        "id": "8NbvXt2ZNjhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def NTXent_loss(out_1, out_2, temperature):\n",
        "    out = torch.cat([out_1, out_2], dim=0)\n",
        "    n_samples = len(out)\n",
        "\n",
        "    # Full similarity matrix\n",
        "    cov = torch.mm(out, out.t().contiguous())\n",
        "    sim = torch.exp(cov / temperature)\n",
        "\n",
        "    mask = ~torch.eye(n_samples, device=sim.device).bool()\n",
        "    neg = sim.masked_select(mask).view(n_samples, -1).sum(dim=-1)\n",
        "\n",
        "    # Positive similarity\n",
        "    pos = torch.exp(torch.sum(out_1 * out_2, dim=-1) / temperature)\n",
        "    pos = torch.cat([pos, pos], dim=0)\n",
        "\n",
        "    loss = -torch.log(pos / neg).mean()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "TfWF0FCeMgDx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation"
      ],
      "metadata": {
        "id": "mKjBM50usi94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "# MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root=\"./data\", train=True,\n",
        "                                           transform=transforms.ToTensor(), \n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root=\"./data\", train=False,\n",
        "                                          transform=transforms.ToTensor(), \n",
        "                                          download=True)\n",
        "\n",
        "# Data augmentation transforms\n",
        "transform_train1 = transforms.Compose([\n",
        "    transforms.RandomRotation(degrees=(-45, 45)),\n",
        "    transforms.ElasticTransform(alpha=10.0),\n",
        "])\n",
        "\n",
        "transform_train2 = transforms.Compose([\n",
        "    transforms.RandomPerspective(distortion_scale=0.7, p=1.0),\n",
        "    transforms.GaussianBlur(kernel_size=(5, 5), sigma=(0.1, 4)),\n",
        "])\n",
        "\n",
        "\n",
        "# Data loaders\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=BATCH_SIZE,\n",
        "                                          shuffle=False)"
      ],
      "metadata": {
        "id": "Y-gDX2ynm12N"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, (images, labels) in enumerate(train_loader):\n",
        "    print(images.shape, labels[0])\n",
        "\n",
        "    fig, ax = plt.subplots(1, 3, figsize = (10, 20))\n",
        "    ax[0].imshow(images[0][0])\n",
        "    ax[1].imshow(transform_train1(images)[0][0])\n",
        "    ax[2].imshow(transform_train2(images)[0][0])\n",
        "    plt.show()\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "gA-ycoVxnHyf",
        "outputId": "ec0c74ba-45ea-4794-fb27-56a232ab2ab5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 1, 28, 28]) tensor(3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x1440 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "l9QzDVG9Iu7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        \n",
        "        # images, labels and transformed images\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        transformedImg1 = transform_train1(images)\n",
        "        transformedImg2 = transform_train2(images)\n",
        "\n",
        "        # print(images.shape)\n",
        "        # print(transformedImg1.shape)\n",
        "\n",
        "        # FORWARD AND BACK PROP\n",
        "        output1 = SimCLRv1_model(transformedImg1)\n",
        "        \n",
        "        output2 = SimCLRv1_model(transformedImg2)\n",
        "\n",
        "        loss = NTXent_loss(output1, output2, TEMPERATURE)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        # UPDATE MODEL PARAMETERS\n",
        "        optimizer.step()\n",
        "        \n",
        "        # LOGGING\n",
        "        if not batch_idx % 50:\n",
        "            print ('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f' \n",
        "                   %(epoch+1, NUM_EPOCHS, batch_idx, \n",
        "                     len(train_loader), loss))\n",
        "    break\n",
        "            "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sVT6Kp4QqokX",
        "outputId": "52b6bb41-41a7-44b7-bc77-a99c9210803c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 64, 7, 7])\n",
            "torch.Size([128, 64, 7, 7])\n",
            "torch.Size([128, 64, 7, 7])\n",
            "torch.Size([128, 64, 7, 7])\n",
            "torch.Size([128, 256, 7, 7])\n",
            "torch.Size([128, 256, 7, 7])\n",
            "torch.Size([128, 64, 7, 7])\n",
            "torch.Size([128, 64, 7, 7])\n",
            "torch.Size([128, 64, 7, 7])\n",
            "torch.Size([128, 64, 7, 7])\n",
            "torch.Size([128, 256, 7, 7])\n",
            "torch.Size([128, 256, 7, 7])\n",
            "torch.Size([128, 64, 7, 7])\n",
            "torch.Size([128, 64, 7, 7])\n",
            "torch.Size([128, 64, 7, 7])\n",
            "torch.Size([128, 64, 7, 7])\n",
            "torch.Size([128, 256, 7, 7])\n",
            "torch.Size([128, 256, 7, 7])\n",
            "torch.Size([128, 128, 7, 7])\n",
            "torch.Size([128, 128, 7, 7])\n",
            "torch.Size([128, 128, 4, 4])\n",
            "torch.Size([128, 128, 4, 4])\n",
            "torch.Size([128, 512, 4, 4])\n",
            "torch.Size([128, 512, 4, 4])\n",
            "torch.Size([128, 128, 4, 4])\n",
            "torch.Size([128, 128, 4, 4])\n",
            "torch.Size([128, 128, 4, 4])\n",
            "torch.Size([128, 128, 4, 4])\n",
            "torch.Size([128, 512, 4, 4])\n",
            "torch.Size([128, 512, 4, 4])\n",
            "torch.Size([128, 128, 4, 4])\n",
            "torch.Size([128, 128, 4, 4])\n",
            "torch.Size([128, 128, 4, 4])\n",
            "torch.Size([128, 128, 4, 4])\n",
            "torch.Size([128, 512, 4, 4])\n",
            "torch.Size([128, 512, 4, 4])\n",
            "torch.Size([128, 128, 4, 4])\n",
            "torch.Size([128, 128, 4, 4])\n",
            "torch.Size([128, 128, 4, 4])\n",
            "torch.Size([128, 128, 4, 4])\n",
            "torch.Size([128, 512, 4, 4])\n",
            "torch.Size([128, 512, 4, 4])\n",
            "torch.Size([128, 256, 4, 4])\n",
            "torch.Size([128, 256, 4, 4])\n",
            "torch.Size([128, 256, 2, 2])\n",
            "torch.Size([128, 256, 2, 2])\n",
            "torch.Size([128, 1024, 2, 2])\n",
            "torch.Size([128, 1024, 2, 2])\n",
            "torch.Size([128, 256, 2, 2])\n",
            "torch.Size([128, 256, 2, 2])\n",
            "torch.Size([128, 256, 2, 2])\n",
            "torch.Size([128, 256, 2, 2])\n",
            "torch.Size([128, 1024, 2, 2])\n",
            "torch.Size([128, 1024, 2, 2])\n",
            "torch.Size([128, 256, 2, 2])\n",
            "torch.Size([128, 256, 2, 2])\n",
            "torch.Size([128, 256, 2, 2])\n",
            "torch.Size([128, 256, 2, 2])\n",
            "torch.Size([128, 1024, 2, 2])\n",
            "torch.Size([128, 1024, 2, 2])\n",
            "torch.Size([128, 256, 2, 2])\n",
            "torch.Size([128, 256, 2, 2])\n",
            "torch.Size([128, 256, 2, 2])\n",
            "torch.Size([128, 256, 2, 2])\n",
            "torch.Size([128, 1024, 2, 2])\n",
            "torch.Size([128, 1024, 2, 2])\n",
            "torch.Size([128, 256, 2, 2])\n",
            "torch.Size([128, 256, 2, 2])\n",
            "torch.Size([128, 256, 2, 2])\n",
            "torch.Size([128, 256, 2, 2])\n",
            "torch.Size([128, 1024, 2, 2])\n",
            "torch.Size([128, 1024, 2, 2])\n",
            "torch.Size([128, 256, 2, 2])\n",
            "torch.Size([128, 256, 2, 2])\n",
            "torch.Size([128, 256, 2, 2])\n",
            "torch.Size([128, 256, 2, 2])\n",
            "torch.Size([128, 1024, 2, 2])\n",
            "torch.Size([128, 1024, 2, 2])\n",
            "torch.Size([128, 512, 2, 2])\n",
            "torch.Size([128, 512, 2, 2])\n",
            "torch.Size([128, 512, 1, 1])\n",
            "torch.Size([128, 512, 1, 1])\n",
            "torch.Size([128, 2048, 1, 1])\n",
            "torch.Size([128, 2048, 1, 1])\n",
            "torch.Size([128, 512, 1, 1])\n",
            "torch.Size([128, 512, 1, 1])\n",
            "torch.Size([128, 512, 1, 1])\n",
            "torch.Size([128, 512, 1, 1])\n",
            "torch.Size([128, 2048, 1, 1])\n",
            "torch.Size([128, 2048, 1, 1])\n",
            "torch.Size([128, 512, 1, 1])\n",
            "torch.Size([128, 512, 1, 1])\n",
            "torch.Size([128, 512, 1, 1])\n",
            "torch.Size([128, 512, 1, 1])\n",
            "torch.Size([128, 2048, 1, 1])\n",
            "torch.Size([128, 2048, 1, 1])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-3aba2acb84f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# FORWARD AND BACK PROP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moutput1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimCLRv1_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformedImg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moutput2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimCLRv1_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformedImg2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-8c1897c10b5f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m         return F.avg_pool2d(input, self.kernel_size, self.stride,\n\u001b[0m\u001b[1;32m    628\u001b[0m                             self.padding, self.ceil_mode, self.count_include_pad, self.divisor_override)\n\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given input size: (2048x1x1). Calculated output size: (2048x-5x-5). Output size is too small"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full Code"
      ],
      "metadata": {
        "id": "BTEys_fRFn2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 - MODEL SETTINGS\n",
        "# Hyperparameters\n",
        "RANDOM_SEED = 1\n",
        "LEARNING_RATE = 0.0001\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "# Architecture\n",
        "NUM_FEATURES = 28*28\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "# LOSS\n",
        "TEMPERATURE = 0.5\n",
        "\n",
        "# Other\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"using : {DEVICE}\")\n",
        "\n",
        "GRAYSCALE = True\n",
        "\n",
        "\n",
        "# 2 - BASE ENCODER\n",
        "from re import X\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        print(out.shape)\n",
        "        out = self.bn1(out)\n",
        "        print(out.shape)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        print(out.shape)\n",
        "\n",
        "        out = self.bn2(out)\n",
        "        print(out.shape)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        print(out.shape)\n",
        "        out = self.bn3(out)\n",
        "        print(out.shape)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes, grayscale):\n",
        "        self.inplanes = 64\n",
        "        if grayscale:\n",
        "            in_dim = 1\n",
        "        else:\n",
        "            in_dim = 3\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_dim, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, (2. / n)**.5)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        print(x.shape)\n",
        "        x = self.bn1(x)\n",
        "        print(x.shape)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        print(x.shape)\n",
        "        x = self.layer2(x)\n",
        "        print(x.shape)\n",
        "        x = self.layer3(x)\n",
        "        print(x.shape)\n",
        "        x = self.layer4(x)\n",
        "        print(x.shape)\n",
        "        # because MNIST is already 1x1 here:\n",
        "        # disable avg pooling\n",
        "        #x = self.avgpool(x)\n",
        "        \n",
        "        x = x.view(x.size(0), -1)\n",
        "        print(x.shape)\n",
        "        logits = self.fc(x)\n",
        "        probas = F.softmax(logits, dim=1)\n",
        "        return logits, probas\n",
        "\n",
        "\n",
        "\n",
        "def resnet50(num_classes):\n",
        "    \"\"\"Constructs a ResNet-50 model.\"\"\"\n",
        "    model = ResNet(block=Bottleneck, \n",
        "                   layers=[3, 4, 6, 3],\n",
        "                   num_classes=NUM_CLASSES,\n",
        "                   grayscale=GRAYSCALE)\n",
        "    return model\n",
        "\n",
        "\n",
        "# 3 - SIMCLR MODEL\n",
        "# SimCLRv1 Model\n",
        "class SimCLR(nn.Module):\n",
        "    def __init__(self, base_encoder, projection_dim=128):\n",
        "        super(SimCLR, self).__init__()\n",
        "\n",
        "        self.encoder = base_encoder\n",
        "        self.projection_head = nn.Sequential(\n",
        "            # self.encoder.fc.in_features Ã  la place de 2048\n",
        "            nn.Linear(2048, 2048),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(2048, projection_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.encoder(x)\n",
        "        z = self.projection_head(h)\n",
        "        return h, z\n",
        "\n",
        "\n",
        "# 4 - LARS OPTIMIZER\n",
        "from flash.core.optimizers import LARS\n",
        "\n",
        "optimizer = LARS(\n",
        "    SimCLRv1_model.parameters(),\n",
        "    lr=0.1,\n",
        "    weight_decay=1e-6,\n",
        "    momentum=0.9\n",
        ")\n",
        "\n",
        "# 5 - NTXENT LOSS\n",
        "def NTXent_loss(out_1, out_2, temperature):\n",
        "    out = torch.cat([out_1, out_2], dim=0)\n",
        "    n_samples = len(out)\n",
        "\n",
        "    # Full similarity matrix\n",
        "    cov = torch.mm(out, out.t().contiguous())\n",
        "    sim = torch.exp(cov / temperature)\n",
        "\n",
        "    mask = ~torch.eye(n_samples, device=sim.device).bool()\n",
        "    neg = sim.masked_select(mask).view(n_samples, -1).sum(dim=-1)\n",
        "\n",
        "    # Positive similarity\n",
        "    pos = torch.exp(torch.sum(out_1 * out_2, dim=-1) / temperature)\n",
        "    pos = torch.cat([pos, pos], dim=0)\n",
        "\n",
        "    loss = -torch.log(pos / neg).mean()\n",
        "    return loss\n",
        "\n",
        "\n",
        "# 6 - DATA AUGMENTATION\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "# MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root=\"./data\", train=True,\n",
        "                                           transform=transforms.ToTensor(), \n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root=\"./data\", train=False,\n",
        "                                          transform=transforms.ToTensor(), \n",
        "                                          download=True)\n",
        "\n",
        "# Data augmentation transforms\n",
        "transform_train1 = transforms.Compose([\n",
        "    transforms.RandomRotation(degrees=(-45, 45)),\n",
        "    transforms.ElasticTransform(alpha=10.0),\n",
        "])\n",
        "\n",
        "transform_train2 = transforms.Compose([\n",
        "    transforms.RandomPerspective(distortion_scale=0.7, p=1.0),\n",
        "    transforms.GaussianBlur(kernel_size=(5, 5), sigma=(0.1, 4)),\n",
        "])\n",
        "\n",
        "\n",
        "# Data loaders\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=BATCH_SIZE,\n",
        "                                          shuffle=False)\n",
        "\n",
        "\n",
        "# 7 - TRAINING LOOP\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        \n",
        "        # images, labels and transformed images\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        transformedImg1 = transform_train1(images)\n",
        "        transformedImg2 = transform_train2(images)\n",
        "\n",
        "        #print(images.shape)\n",
        "        #print(transformedImg1)\n",
        "        \n",
        "        # FORWARD AND BACK PROP\n",
        "        output1 = SimCLRv1_model(transformedImg1)\n",
        "        \n",
        "        output2 = SimCLRv1_model(transformedImg2)\n",
        "\n",
        "        loss = NTXent_loss(output1, output2, TEMPERATURE)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        # UPDATE MODEL PARAMETERS\n",
        "        optimizer.step()\n",
        "        \n",
        "        # LOGGING\n",
        "        if not batch_idx % 50:\n",
        "            print ('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f' \n",
        "                   %(epoch+1, NUM_EPOCHS, batch_idx, \n",
        "                     len(train_loader), loss))\n",
        "            \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "il4aTuZhskBe",
        "outputId": "45481928-10d3-4794-9a61-d67383cf3fcf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using : cpu\n"
          ]
        }
      ]
    }
  ]
}